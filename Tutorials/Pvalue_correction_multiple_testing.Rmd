---
title: "P-value correction"
author: "N. Schenk"
date: "2022-10-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Correcting for multiple testing
Aim of this document : summary of current recommendations for Ecology based on Literature and discussion within the Synthesis Team, and answers to questions around the topic that have been asked to the Synthesis helpdesk.

## Description of the problem
If we work with a p-value threshold of 0.05, we are controlling for false positives. A p-value of 0.05 tells us : If there was no effect (H0 is true), the probability of observing our data (a value for the test statistic as extreme or more extreme than ours) is 5%. There is a 5% chance of observing data at least as extreme than ours, by chance alone. The idea of the p-value is to safeguards us from seeing an effect where there is actually none, it prevents us from assuming biological importance when there is actually none. These "wrong discoveries" are called type-I-errors. The p-value controls for the probability of type-I-errors.


## Multiple testing
A p-value of lower than 5% is telling us that the chance of observing our (or more extreme) data just by chance is 5%. In other words, in 1 of 20 cases, we would observe data at least as extreme as ours just by chance (5% * 20 trials = 100%).
In many cases, we do test the same question more than once with the dataset, and if the number of cases exceeds a "reasonable number*" (coming back to that later)#TODO, we expect many of our significant results to be just a product of chance. We expect that we will have some false positive results among our significant results.

ADD HERE #TODO jellybean example

Sources so far : (from Colquhoun 2014 https://doi.org/10.1098/rsos.140216), (from Journal Club Muff presentation), (https://www.publichealth.columbia.edu/research/population-health-methods/false-discovery-rate)

Multiple testing occurs if you test (almost) the same question with the same data multiple times. In a paper, you usually test more than one research question with the same data, that is not a problem. The problem is if one or more of your questions are tested multiple times. This can occurr if you have more than one choice of indicator for the same process (e.g. the SMI and the FORMI in forests), and you want to test all of them. Or if you have several potential weightings of a variable, and want to use all of them (e.g. abundance weightings of species diversity). If these cases only lead to 2 or 3 models, you don't have to correct for multiple testing. But if you have a combination of those cases, the number of models can quickly exceed X (#TODO search rule of thumb).

Potential cases are : 
- Using correlated response variables, e.g. 
    - both the FORMI and the SMI as forest management indices, plus various weightings in your explanatory 


# QUICK : Best practice for multiple testing
The recommended procedure for ecologists is to use the Benjamini-Hochberg (REF #TODO) method (Waite 2006 #TODO).
False Discovery Rate = q-values (#TODO check)

Implementation in R : 
1. Run all your hypothesis tests
2. feed all the p-values in a vector (and remember which pvalue belongs to which analysis and variable, e.g. by using named vectors or a data.frame)
3. Convert the p-values to q-values (#TODO check), using the function `stats::p.adjust()` by the `stats` package in `R`.
4. Use a 5% treshold for the q-values (#TODO check). You will find a lower number of significant results.
```{r}
p_sim <- c(0.03, 0.04, 0.03, 0.05, 0.05, 0.001, 0.002, 0.003, 0.005, 0.003, 0.1, 0.2, 0.035, 0.055, 0.045)
bh_fdr <- stats::p.adjust(p_sim, method = "fdr") # is equal to method = "BH"
```



**FAQ**
## Which p-values should I correct together?
If I want to correct for multiple testing in my analysis, do I only correct the p-values of similar models, or all p-values of the paper?

You need to correct all p-values that were produced from testing **the same research question**. E.e. if you have different abundance weightings (e.g. q : 0, 1, 2) of alpha diversity (species richness) of several groups (bacteria, algae, fungi), you take the p-values from all those models.
Because : same explanatory varaibles used!
I should #TODO write up the models so it's more clear



Welche p-Werte sollen korrigiert werden? alle eines papers? oder nur alle, die die selbe Frage stellen?
Siehe L. Email #TODO
 P-value corrections : Which p-values should we correct (and should we)?
        e.g. among the alpha models : only within algae, or across all 3 groups? Across all q values? 
    Answer :
        Yes, we should better do it.
        correct q values by question, ideally correct all the p-values of all alpha models together, i.e. the p-values of algae q0, algae q1, fungi q0, fungi q1, bac q0, bacq1, algae q2, ...   --> So in the end, there are two long vectors of p-values to be corrected. (And yes, the more p-values we correct, the less significant effects we will get, but see below)


## Ab wie vielen Tests sollte ich die p-werte korrigieren? 
Faustregel suchen #TODO




Topic arosen during discussion between Lukas Dreyling, Caterina Penone and Noelle Schenk about manuscript Dreyling et. al, unpublished in July 2022.

Question 1 : p-value corrections

   
        Caterina applied a p-value correction in this paper (https://doi.org/10.1111/ele.13182), citing the Verhoeven paper https://doi.org/10.1111/j.0030-1299.2005.13727.x
        She used the same R function as we used : stats::p.adjust(p_sim, method = "fdr")
        She also adjusted the FDR level, i.e. she changed the threshold of significant to 0.2 AFTER correcting the p-value. After the "FDR" = "Benjamini-Hochberg" correction, the p-value can be interpreted as false discovery rate, and we can think about a meaningful level for the fdr. E.g. in her paper she argued that she should really not miss effects, and she can live with a 20% chance that effects are wrongly detected as significant, but in fact are not. 


From package description of the `p.adjust` function : 

## Benjamini Hochberg, FDR
- The "BH" (aka "fdr") and "BY" methods of Benjamini, Hochberg, and Yekutieli control the false discovery rate, the expected proportion of false discoveries amongst the rejected hypotheses. The false discovery rate is a less stringent condition than the family-wise error rate, so these methods are more powerful than the others.
- In order to be able to identify as many significant comparisons as possible while still maintaining a low false positive rate, the False Discovery Rate (FDR) and its analog the q-value are utilized. (https://www.publichealth.columbia.edu/research/population-health-methods/false-discovery-rate)
- - The FDR is the rate that features called significant are truly null. An FDR of 5% means that, among all features called significant, 5% of these are truly null (https://www.publichealth.columbia.edu/research/population-health-methods/false-discovery-rate)
- A q-value threshold of 0.05 yields a FDR of 5% among all features called significant. The q-value is the expected proportion of false positives among all features as or more extreme than the observed one. This is particularly useful when we wish to make a large number of discoveries for further confirmation later on (i.e. pilot study or exploratory analyses, for example if we did a gene expression microarray to pick differentially expressed genes for confirmation with real-time PCR). (https://www.publichealth.columbia.edu/research/population-health-methods/false-discovery-rate)
- also called **q-value**
- To reduce potential type I errors associated with multiple testing while minimising type II errors, we controlled for false discovery rates (FDR) using a Benjaminiâ€“Hochberg procedure with a threshold of 0.2 (Verhoeven et al. 2005) (sentence from Penone 2019)





words describing same process (FDR related) : false positive, false discovery rate, error rate




## changing p-value level
In promoting the BH method as an alternative to
Bonferroni-based methods, we do not mean to imply that
correcting for multiplicity is universally appropriate. In
some cases, it may be compelling to control neither FWER
nor FDR (Roback & Askins, 2005). Instead, it may be
appropriate to simply interpret each nominal P-value with-
out regard to the number of tests conducted. (It may even be
appropriate to set the critical Î±-level higher than the conven-
tional 0.05.) Imagine, for instance, an exploratory analysis
aimed at identifying correlates of extinction risk


dummy example of code :
```{r}
p_sim <- c(0.03, 0.04, 0.03, 0.05, 0.05, 0.001, 0.002, 0.003, 0.005, 0.003, 0.1, 0.2, 0.035, 0.055, 0.045)
bh_fdr <- stats::p.adjust(p_sim, method = "fdr") # is equal to method = "BH"
plot(p_sim, bh_fdr)
abline(v = 0.05)
# applying a 0.2 threshold (exploratory study)
names(bh_fdr)[bh_fdr >= 0.2] <- "n.s"
names(bh_fdr)[bh_fdr < 0.2] <- "*"
```
There is also a function attached which does this.
 

        So for your study, it would be possible to argue in this direction, too : The system is fully unknown --> better we detect something as significant than if we miss something. You could e.g. lift the significance level (=the FDR level) to 0.1 or we could discuss.
        But of corse, we need to set the level before correcting the p-values ðŸ˜Š And if the new FDR level is too high, we could happen to get more significant effects than before correcting, so it should really not be too high.
        The reasoning is better explained in the Verhoeven paper, and we two also need to read and think about it a bit more, and then maybe discuss, or what would you prefer? 
